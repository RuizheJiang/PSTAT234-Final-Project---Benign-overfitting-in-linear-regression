{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benign Overfitting in Linear Regression: Experimental Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern over-parameterized models can perfectly fit training data yet still generalize well, a phenomenon known as benign overfitting​. In classical theory, a model that interpolates noisy data would severely overfit and perform poorly on new data. However, recent work by Bartlett et al. (2020) provides conditions under which linear regression can interpolate noise benignly, achieving near-optimal test accuracy despite overfitting the training data​. Two key insights from their analysis are:\n",
    "\n",
    "- Significant Over-parameterization: The model must have far more parameters than samples. In fact, the number of “uninformative” directions in parameter space (directions with little effect on prediction) should significantly exceed the sample size​. This ensures there are many degrees of freedom to absorb noise without affecting predictive features.\n",
    "Slowly Decaying Eigenvalues (High Effective Rank): The covariance spectrum of the features should not drop off too fast. Intuitively, there should be a large effective rank for the feature covariance – meaning many small-eigenvalue directions where label noise can hide with minimal impact on predictions​. If the small eigenvalues decay slowly (e.g. a heavy-tailed spectrum), the minimum-norm interpolating solution will spread out the fitted noise in many weak directions, limiting its harm on test error.\n",
    "\n",
    "- Slowly Decaying Eigenvalues (High Effective Rank): The covariance spectrum of the features should not drop off too fast. Intuitively, there should be a large effective rank for the feature covariance – meaning many small-eigenvalue directions where label noise can hide with minimal impact on predictions​. If the small eigenvalues decay slowly (e.g. a heavy-tailed spectrum), the minimum-norm interpolating solution will spread out the fitted noise in many weak directions, limiting its harm on test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Setup: Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study these phenomena in a controlled way, we generate synthetic regression data where we can specify the feature covariance structure. We consider a linear model:\n",
    "- Data $(x_i, y_i)$ for $i=1,\\dots,n$ are drawn i.i.d. in $\\mathbb{R}^p \\times \\mathbb{R}$.\n",
    "- Feature vector $x_i \\sim \\mathcal{N}(0, \\Sigma)$, a zero-mean Gaussian with covariance $\\Sigma$. We will design $\\Sigma$ to have various eigenvalue spectra (e.g. slow or fast decaying eigenvalues, correlated features, etc.).\n",
    "- The true underlying relationship is $y = x^\\top w^* + \\varepsilon$, where $w^*$ is the true parameter vector and $\\varepsilon$ is noise $\\sim \\mathcal{N}(0, \\sigma^2)$. In our experiments, we set $\\sigma^2=1$ for simplicity (so noise variance is 1). We will often take $w^ = 0$ to focus purely on the effect of fitting noise. (When $w^*=0$, the best possible prediction is $0$ for all $x$, with mean squared error equal to the noise variance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit a linear regression model in the highly over-parameterized regime ($p \\ge n$) using the minimum-norm interpolating solution – essentially the pseudoinverse solution that fits $y$ exactly. This is the solution of $\\min_{w}|w|_2$ subject to $Xw = y$, where $X$ is the $n\\times p$ design matrix. This choice aligns with the theoretical analysis (it’s the estimator that many gradient-based methods would pick in an underdetermined system). \n",
    "\n",
    "Evaluation: For each experiment, we will measure:\n",
    "\n",
    "- Training MSE: The mean squared error on the training set (which will be near 0 in over-parameterized cases when interpolation is achieved).\n",
    "- Test MSE: The mean squared error on an independent test set, which indicates generalization performance. For comparison, note that the irreducible error (noise variance) is 1 in our setup, and if $w^*$ is non-zero, the Bayes-optimal predictor $w^*$ would achieve a test MSE equal to the noise variance (plus any approximation error if $w^*$ not realizable, but here $w^*$ is the true linear parameter so noise is the only source of error). Benign overfitting means the test MSE of the fitted model should be close to this optimum despite fitting noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we implement helper functions to generate synthetic data with a specified covariance spectrum and to compute the minimum-norm solution and errors. We will use these throughout our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE (should be ~0 if p>n): 8.41e-30,  Test MSE (example): 0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data(n, p, eigen_vals, w_star=None, correlated=True, random_state=None):\n",
    "    \"\"\"\n",
    "    Generate synthetic linear regression data (X, y) with given covariance eigenvalues.\n",
    "    - eigen_vals: array of length p specifying the eigenvalues of covariance Sigma.\n",
    "    - If correlated=True, features are correlated (Sigma is not diagonal), generated via a random orthonormal basis.\n",
    "      If correlated=False, features will be independent with variances = eigen_vals (Sigma diagonal).\n",
    "    - w_star: true underlying parameter vector (length p). If None, uses zeros (so y is pure noise).\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    eigen_vals = np.array(eigen_vals)\n",
    "    p = len(eigen_vals)\n",
    "    # Construct covariance matrix Sigma = Q * diag(eigen_vals) * Q^T\n",
    "    if correlated:\n",
    "        # random orthonormal matrix for eigenvectors\n",
    "        Q, _ = np.linalg.qr(np.random.randn(p, p))\n",
    "    else:\n",
    "        # Use identity as eigenvector basis (so features independent)\n",
    "        Q = np.eye(p)\n",
    "    Sigma = Q @ np.diag(eigen_vals) @ Q.T\n",
    "    # Generate data: X ~ N(0, Sigma) with n samples\n",
    "    # We can sample X by drawing Z ~ N(0, I_p) and then transforming: X = Z * Sigma^(1/2).\n",
    "    # Compute a Cholesky factor for Sigma for sampling:\n",
    "    try:\n",
    "        L = np.linalg.cholesky(Sigma)\n",
    "    except np.linalg.LinAlgError:\n",
    "        # If Sigma is nearly singular, add a tiny diagonal for stability\n",
    "        L = np.linalg.cholesky(Sigma + 1e-10 * np.eye(p))\n",
    "    # Each row of X is generated as (Z_i * L) where Z_i ~ N(0, I_p)\n",
    "    Z = np.random.randn(n, p)  # n x p standard normal\n",
    "    X = Z @ L.T               # X will be n x p, with Cov(X_row) = Sigma\n",
    "    # True parameter w_star (if not given, default to zero vector)\n",
    "    if w_star is None:\n",
    "        w_star = np.zeros(p)\n",
    "    # Sample noise \n",
    "    noise = np.random.randn(n)\n",
    "    # Generate labels: y = X w* + noise\n",
    "    y = X.dot(w_star) + noise\n",
    "    return X, y, w_star, Sigma\n",
    "\n",
    "def fit_min_norm(X, y):\n",
    "    \"\"\"Find the minimum-norm solution (pseudoinverse solution) for X w = y.\"\"\"\n",
    "    # Use np.linalg.lstsq with rcond=None to obtain the minimum-norm solution in underdetermined cases\n",
    "    w_min_norm, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
    "    return w_min_norm\n",
    "\n",
    "def compute_mse(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "def evaluate_model(w_hat, w_true, Sigma):\n",
    "    \"\"\"\n",
    "    Compute train MSE on the training data (assuming we have access to X_train, y_train globally or closure)\n",
    "    and estimate test MSE given model w_hat and true w_true and covariance Sigma.\n",
    "    We generate a fresh test sample from N(0, Sigma) (with same noise variance 1).\n",
    "    \"\"\"\n",
    "    # Use global X_train, y_train if available (in this context, we'll call evaluate_model inside the same cell where X, y are defined)\n",
    "    train_pred = X_train.dot(w_hat)\n",
    "    train_mse = compute_mse(train_pred, y_train)\n",
    "    # Generate a large test set for reliable estimate\n",
    "    n_test = 10000\n",
    "    Zt = np.random.randn(n_test, Sigma.shape[0])\n",
    "    X_test = Zt @ np.linalg.cholesky(Sigma).T\n",
    "    y_test = X_test.dot(w_true) + np.random.randn(n_test)\n",
    "    test_pred = X_test.dot(w_hat)\n",
    "    test_mse = compute_mse(test_pred, y_test)\n",
    "    return train_mse, test_mse\n",
    "\n",
    "# Quick demonstration of usage (this is not an experiment yet, just sanity check)\n",
    "n, p = 50, 100\n",
    "eigs = np.ones(p)  # identity covariance (all eigenvalues 1)\n",
    "X_train, y_train, w_true, Sigma = generate_data(n, p, eigs, w_star=None, correlated=False, random_state=42)\n",
    "w_hat = fit_min_norm(X_train, y_train)\n",
    "train_mse, test_mse = compute_mse(X_train.dot(w_hat), y_train), compute_mse(X_train.dot(w_hat), y_train)\n",
    "print(f\"Train MSE (should be ~0 if p>n): {train_mse:.2e},  Test MSE (example): {test_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, generate_data allows us to specify a list of eigenvalues for $\\Sigma$ and whether features are independent or correlated. If correlated=False, $\\Sigma$ is taken as diagonal (features independent with given variances). If correlated=True, we generate a random orthonormal matrix $Q$ to produce a full covariance $Q \\operatorname{diag}(eigen\\_vals) Q^T$ with the desired spectrum but mixed features. We use Cholesky decomposition to sample $X$ efficiently from the Gaussian distribution. The helper fit_min_norm returns the minimum-norm interpolating weight vector $w_{\\min}$ for the linear system $Xw=y$. Finally, evaluate_model will be used to compute training and test MSE given a learned $w_{\\hat{}}$ and true $w^*$ (note: we will call evaluate_model in the same scope where X_train, y_train are defined for convenience)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benign Overfitting under Ideal Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we construct a scenario expected to satisfy the benign overfitting conditions. According to theory, we need: (a) significant overparameterization ($p \\gg n$), and (b) a slowly decaying covariance spectrum with many small eigenvalues (high effective rank). We will use:\n",
    "- Sample size: $n = 100$\n",
    "- Feature dimension: $p = 1000$ (an order of magnitude larger than $n$, providing plenty of extra degrees of freedom)\n",
    "- Covariance spectrum: For an ideal slow-decay scenario, we can take approximately constant eigenvalues. The most extreme case of “slow decay” is no decay at all – e.g. $\\lambda_i \\approx 1$ for all $i$. Here we’ll use $\\Sigma = I$ (identity covariance) as a simple case where all eigenvalues are equal (1). This means features are independent and of equal importance, and effectively the rank = p = 1000 (maximal effective rank)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also set the true signal $w^* = 0$ so that labels are pure noise. This way, any test error above the noise variance clearly indicates overfitting harm, whereas achieving test MSE $\\approx 1$ (the noise variance) would be essentially optimal (since even the best predictor, which is $0$, has to incur the noise MSE of 1). This lets us directly assess if fitting the noise has increased test error or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation: In this benign setting, the minimum-norm interpolator should achieve near-noise-level test MSE despite zero training error. As Bartlett et al. noted, if there are many “unimportant” directions (here 1000-100 = 900 extra directions) with small variance, the fitted noise will reside in those directions and not impact prediction much​."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE = 3.35e-30\n",
      "Test MSE  = 1.085 (noise variance = 1.0)\n"
     ]
    }
   ],
   "source": [
    "# Benign scenario: p >> n and flat (slow-decay) spectrum\n",
    "n = 100\n",
    "p = 1000\n",
    "eigen_vals = np.ones(p)   # all eigenvalues = 1 (identity covariance)\n",
    "X_train, y_train, w_true, Sigma = generate_data(n, p, eigen_vals, w_star=np.zeros(p), correlated=False, random_state=1)\n",
    "w_hat = fit_min_norm(X_train, y_train)\n",
    "train_mse = compute_mse(X_train.dot(w_hat), y_train)\n",
    "# Estimate test MSE on a large test set\n",
    "n_test = 10000\n",
    "X_test = np.random.randn(n_test, p)  # since Sigma=I, we can sample standard normal for features\n",
    "y_test = X_test.dot(w_true) + np.random.randn(n_test)\n",
    "test_mse = compute_mse(X_test.dot(w_hat), y_test)\n",
    "print(f\"Train MSE = {train_mse:.2e}\")\n",
    "print(f\"Test MSE  = {test_mse:.3f} (noise variance = 1.0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: The output shows the training MSE is essentially 0, confirming the model interpolates the training data perfectly. The test MSE should come out around ~1.1. This is very close to 1.0, the noise level. Despite fitting pure noise in the training set, the model’s predictions on new data are nearly as good as always predicting the mean (zero in this case). The small excess over 1 can be attributed to finite sample fluctuations, but it diminishes as $p$ grows even larger. We have effectively hidden the noise in the many extra dimensions of $w_{\\hat{}}$ that correspond to tiny (here zero) marginal variance in $x$, thus not hurting prediction on new samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the effect of over-parameterization more clearly, let’s vary the model dimension $p$ and see how test performance changes. We’ll keep $n=100$ fixed and use the same identity covariance (eigenvalues all 1), and examine test MSE for different ratios of $p/n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p =  100, Test MSE ≈ 27813.00\n",
      "p =  150, Test MSE ≈ 3.37\n",
      "p =  300, Test MSE ≈ 1.51\n",
      "p =  500, Test MSE ≈ 1.30\n",
      "p = 1000, Test MSE ≈ 1.11\n",
      "p = 2000, Test MSE ≈ 1.07\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "n = 100\n",
    "p_values = [100, 150, 300, 500, 1000, 2000]  # from equal to n up to 20x n\n",
    "test_mse_results = []\n",
    "for p in p_values:\n",
    "    # average test MSE over a few random trials for stability\n",
    "    trials = 3\n",
    "    mse_sum = 0.0\n",
    "    for seed in range(trials):\n",
    "        X_train, y_train, w_true, Sigma = generate_data(n, p, np.ones(p), w_star=np.zeros(p), correlated=False, random_state=seed)\n",
    "        w_hat = fit_min_norm(X_train, y_train)\n",
    "        # Compute test MSE\n",
    "        X_test = np.random.randn(10000, p)  # sampling from N(0,I) for test\n",
    "        y_test = X_test.dot(w_true) + np.random.randn(10000)\n",
    "        mse_sum += compute_mse(X_test.dot(w_hat), y_test)\n",
    "    avg_test_mse = mse_sum / trials\n",
    "    test_mse_results.append(avg_test_mse)\n",
    "    print(f\"p = {p:4d}, Test MSE ≈ {avg_test_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect a trend: when $p$ is only equal to or slightly above $n$, test MSE will be significantly higher than 1 (indicating harmful overfitting), but as $p$ grows much larger than $n$, test MSE drops toward 1. The printed results confirm this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trend confirms that significant overparameterization is crucial: as the number of features grows well beyond the sample size, the test error drops to the noise floor. This matches the theory that a large surplus of parameters (here, hundreds more dimensions than data points) is required for benign overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Role of Effective Rank: Varying the Covariance Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we investigate the effect of the covariance eigenvalue decay on benign overfitting. The theory’s characterization is in terms of the effective rank of $\\Sigma$​. Intuitively, even if $p$ is large, if the feature covariance has most of its variance concentrated in a small number of top directions (i.e. eigenvalues that decay rapidly), then there aren’t enough “small-variance” directions to hide the noise. The small eigenvalues need to decay slowly, so that there are many of them contributing a significant collective variance (making the effective rank large)​. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we will keep $n=100$ and $p=1000$ (so plenty of dimensions), but we will design different covariance spectra from slow-decaying to fast-decaying. We will use independent features for clarity (correlated=False) so that $\\Sigma = \\operatorname{diag}(\\lambda_1,\\dots,\\lambda_p)$ with the desired eigenvalues. \n",
    "\n",
    "We compare:\n",
    "- Slow Decay: Eigenvalues that decrease gradually. For example, $\\lambda_i \\propto \\frac{1}{i}$ (harmonic decay) is fairly slow. We could also consider a flat spectrum (which we did above: constant eigenvalues) as an extreme case of slow/no decay.\n",
    "- Moderate Decay: Eigenvalues might decay polynomially faster or have a mix of large and small values. For instance, $\\lambda_i \\propto \\frac{1}{i^2}$ decays faster.\n",
    "- Fast Decay: Eigenvalues that drop off very quickly, e.g. an exponential decay $\\lambda_i \\propto a^i$ for some $a<1$. This means after the first few directions, the remaining eigenvalues (and corresponding feature directions) carry negligible variance. This is a scenario with low effective rank, as most variance is in the top components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will simulate these cases and measure test MSE of the min-norm interpolator in each case. All cases use $p=1000$, $n=100$, and $w^*=0$ (pure noise labels) to isolate the effect on fitting noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected results: The test MSE will be lowest for the flattest/slowest-decaying spectrum and highest for the fastest-decaying spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrum: flat (all equal)     -> Test MSE ≈ 1.15\n",
      "Spectrum: slow (1/i)           -> Test MSE ≈ 1.36\n",
      "Spectrum: moderate (1/i^2)     -> Test MSE ≈ 2.00\n",
      "Spectrum: fast (exp decay)     -> Test MSE ≈ 4.96\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "p = 1000\n",
    "# Define different eigenvalue spectra\n",
    "eigen_spectra = {\n",
    "    \"flat (all equal)\": np.ones(p),\n",
    "    \"slow (1/i)\": 1.0 / (np.arange(p) + 1),             # harmonic decay\n",
    "    \"moderate (1/i^2)\": 1.0 / ((np.arange(p) + 1)**2),   # polynomial decay (faster)\n",
    "    \"fast (exp decay)\": 0.95**(np.arange(p))             # exponential decay with ratio 0.95\n",
    "}\n",
    "# Function to get test MSE for a given spectrum\n",
    "def get_test_mse_for_spectrum(eigs):\n",
    "    X_train, y_train, w_true, Sigma = generate_data(n, p, eigs, w_star=np.zeros(p), correlated=False)\n",
    "    w_hat = fit_min_norm(X_train, y_train)\n",
    "    # Evaluate test error on large test set\n",
    "    X_test = np.random.randn(10000, p) * np.sqrt(eigs)  # since features independent, we can multiply std dev\n",
    "    # (Alternatively, generate from Sigma via generate_data for test)\n",
    "    y_test = X_test.dot(w_true) + np.random.randn(10000)\n",
    "    test_mse = compute_mse(X_test.dot(w_hat), y_test)\n",
    "    return test_mse\n",
    "\n",
    "# Calculate test MSE for each spectrum\n",
    "for desc, eigs in eigen_spectra.items():\n",
    "    mse = get_test_mse_for_spectrum(eigs)\n",
    "    print(f\"Spectrum: {desc:20s} -> Test MSE ≈ {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers (illustrative) show a clear pattern: when eigenvalues decay very fast (exponential case), the model that interpolates noise performs very poorly on test data (MSE several times the noise level). Conversely, with a flat or slowly decaying spectrum, the test MSE is close to 1.0 (noise level), indicating benign behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This aligns with the effective rank condition from the paper: a large effective rank (many comparably small eigenvalues) is necessary to keep the excess error from noise small​. In the exponential decay case, most of the variance is in the first few features; the model must place a lot of weight on those few directions to fit the noise (because the remaining directions have near-zero variance in $x$ and thus can’t absorb much noise without huge weights). Fitting noise in those important directions severely hurts predictions, hence the high test error. On the other hand, in the slow-decay cases, there are lots of directions with modest variance – the noise gets spread out over many directions of $w_{\\hat{}}$, each of which has little influence on the prediction, keeping test error low。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, if the covariance spectrum decays too quickly, benign overfitting fails: the interpolating solution will generalize poorly because it overfits in the important directions. Only when the spectrum has a long tail (slow decay) can the overfitting be harmless."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
