{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benign Overfitting in Linear Regression: Experimental Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern over-parameterized models can perfectly fit training data yet still generalize well, a phenomenon known as benign overfitting​. In classical theory, a model that interpolates noisy data would severely overfit and perform poorly on new data. However, recent work by Bartlett et al. (2020) provides conditions under which linear regression can interpolate noise benignly, achieving near-optimal test accuracy despite overfitting the training data​. Two key insights from their analysis are:\n",
    "\n",
    "- Significant Over-parameterization: The model must have far more parameters than samples. In fact, the number of “uninformative” directions in parameter space (directions with little effect on prediction) should significantly exceed the sample size​. This ensures there are many degrees of freedom to absorb noise without affecting predictive features.\n",
    "Slowly Decaying Eigenvalues (High Effective Rank): The covariance spectrum of the features should not drop off too fast. Intuitively, there should be a large effective rank for the feature covariance – meaning many small-eigenvalue directions where label noise can hide with minimal impact on predictions​. If the small eigenvalues decay slowly (e.g. a heavy-tailed spectrum), the minimum-norm interpolating solution will spread out the fitted noise in many weak directions, limiting its harm on test error.\n",
    "\n",
    "- Slowly Decaying Eigenvalues (High Effective Rank): The covariance spectrum of the features should not drop off too fast. Intuitively, there should be a large effective rank for the feature covariance – meaning many small-eigenvalue directions where label noise can hide with minimal impact on predictions​. If the small eigenvalues decay slowly (e.g. a heavy-tailed spectrum), the minimum-norm interpolating solution will spread out the fitted noise in many weak directions, limiting its harm on test error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Setup: Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study these phenomena in a controlled way, we generate synthetic regression data where we can specify the feature covariance structure. We consider a linear model:\n",
    "- Data $(x_i, y_i)$ for $i=1,\\dots,n$ are drawn i.i.d. in $\\mathbb{R}^p \\times \\mathbb{R}$.\n",
    "- Feature vector $x_i \\sim \\mathcal{N}(0, \\Sigma)$, a zero-mean Gaussian with covariance $\\Sigma$. We will design $\\Sigma$ to have various eigenvalue spectra (e.g. slow or fast decaying eigenvalues, correlated features, etc.).\n",
    "- The true underlying relationship is $y = x^\\top w^* + \\varepsilon$, where $w^*$ is the true parameter vector and $\\varepsilon$ is noise $\\sim \\mathcal{N}(0, \\sigma^2)$. In our experiments, we set $\\sigma^2=1$ for simplicity (so noise variance is 1). We will often take $w^ = 0$ to focus purely on the effect of fitting noise. (When $w^*=0$, the best possible prediction is $0$ for all $x$, with mean squared error equal to the noise variance.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit a linear regression model in the highly over-parameterized regime ($p \\ge n$) using the minimum-norm interpolating solution – essentially the pseudoinverse solution that fits $y$ exactly. This is the solution of $\\min_{w}|w|_2$ subject to $Xw = y$, where $X$ is the $n\\times p$ design matrix. This choice aligns with the theoretical analysis (it’s the estimator that many gradient-based methods would pick in an underdetermined system). \n",
    "\n",
    "Evaluation: For each experiment, we will measure:\n",
    "\n",
    "- Training MSE: The mean squared error on the training set (which will be near 0 in over-parameterized cases when interpolation is achieved).\n",
    "- Test MSE: The mean squared error on an independent test set, which indicates generalization performance. For comparison, note that the irreducible error (noise variance) is 1 in our setup, and if $w^*$ is non-zero, the Bayes-optimal predictor $w^*$ would achieve a test MSE equal to the noise variance (plus any approximation error if $w^*$ not realizable, but here $w^*$ is the true linear parameter so noise is the only source of error). Benign overfitting means the test MSE of the fitted model should be close to this optimum despite fitting noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we implement helper functions to generate synthetic data with a specified covariance spectrum and to compute the minimum-norm solution and errors. We will use these throughout our experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE (should be ~0 if p>n): 8.41e-30,  Test MSE (example): 0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_data(n, p, eigen_vals, w_star=None, correlated=True, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    eigen_vals = np.array(eigen_vals)\n",
    "    p = len(eigen_vals)\n",
    "    # Construct covariance matrix Sigma = Q * diag(eigen_vals) * Q^T\n",
    "    if correlated:\n",
    "        # random orthonormal matrix for eigenvectors\n",
    "        Q, _ = np.linalg.qr(np.random.randn(p, p))\n",
    "    else:\n",
    "        # Use identity as eigenvector basis (so features independent)\n",
    "        Q = np.eye(p)\n",
    "    Sigma = Q @ np.diag(eigen_vals) @ Q.T\n",
    "    # Generate data: X ~ N(0, Sigma) with n samples\n",
    "    # We can sample X by drawing Z ~ N(0, I_p) and then transforming: X = Z * Sigma^(1/2).\n",
    "    # Compute a Cholesky factor for Sigma for sampling:\n",
    "    try:\n",
    "        L = np.linalg.cholesky(Sigma)\n",
    "    except np.linalg.LinAlgError:\n",
    "        # If Sigma is nearly singular, add a tiny diagonal for stability\n",
    "        L = np.linalg.cholesky(Sigma + 1e-10 * np.eye(p))\n",
    "    # Each row of X is generated as (Z_i * L) where Z_i ~ N(0, I_p)\n",
    "    Z = np.random.randn(n, p)  # n x p standard normal\n",
    "    X = Z @ L.T                # X will be n x p, with Cov(X_row) = Sigma\n",
    "    # True parameter w_star (if not given, default to zero vector)\n",
    "    if w_star is None:\n",
    "        w_star = np.zeros(p)\n",
    "    # Sample noise \n",
    "    noise = np.random.randn(n)\n",
    "    # Generate labels: y = X w* + noise\n",
    "    y = X.dot(w_star) + noise\n",
    "    return X, y, w_star, Sigma\n",
    "\n",
    "def fit_min_norm(X, y):\n",
    "    \"\"\"Find the minimum-norm solution (pseudoinverse solution) for X w = y.\"\"\"\n",
    "    w_min_norm, *_ = np.linalg.lstsq(X, y, rcond=None)\n",
    "    return w_min_norm\n",
    "\n",
    "def compute_mse(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "def evaluate_model(w_hat, w_true, Sigma):\n",
    "    # Use global X_train, y_train if available (in this context, we'll call evaluate_model inside the same cell where X, y are defined)\n",
    "    train_pred = X_train.dot(w_hat)\n",
    "    train_mse = compute_mse(train_pred, y_train)\n",
    "    # Generate a large test set for reliable estimate\n",
    "    n_test = 10000\n",
    "    Zt = np.random.randn(n_test, Sigma.shape[0])\n",
    "    X_test = Zt @ np.linalg.cholesky(Sigma).T\n",
    "    y_test = X_test.dot(w_true) + np.random.randn(n_test)\n",
    "    test_pred = X_test.dot(w_hat)\n",
    "    test_mse = compute_mse(test_pred, y_test)\n",
    "    return train_mse, test_mse\n",
    "\n",
    "# Quick demonstration of usage (this is not an experiment yet, just sanity check)\n",
    "n, p = 50, 100\n",
    "eigs = np.ones(p)  # identity covariance (all eigenvalues 1)\n",
    "X_train, y_train, w_true, Sigma = generate_data(n, p, eigs, w_star=None, correlated=False, random_state=42)\n",
    "w_hat = fit_min_norm(X_train, y_train)\n",
    "train_mse, test_mse = compute_mse(X_train.dot(w_hat), y_train), compute_mse(X_train.dot(w_hat), y_train)\n",
    "print(f\"Train MSE (should be ~0 if p>n): {train_mse:.2e},  Test MSE (example): {test_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code above, generate_data allows us to specify a list of eigenvalues for $\\Sigma$ and whether features are independent or correlated. If correlated=False, $\\Sigma$ is taken as diagonal (features independent with given variances). If correlated=True, we generate a random orthonormal matrix $Q$ to produce a full covariance $Q \\operatorname{diag}(eigen\\_vals) Q^T$ with the desired spectrum but mixed features. We use Cholesky decomposition to sample $X$ efficiently from the Gaussian distribution. The helper fit_min_norm returns the minimum-norm interpolating weight vector $w_{\\min}$ for the linear system $Xw=y$. Finally, evaluate_model will be used to compute training and test MSE given a learned $w_{\\hat{}}$ and true $w^*$ (note: we will call evaluate_model in the same scope where X_train, y_train are defined for convenience)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benign Overfitting under Ideal Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we construct a scenario expected to satisfy the benign overfitting conditions. According to theory, we need: (a) significant overparameterization ($p \\gg n$), and (b) a slowly decaying covariance spectrum with many small eigenvalues (high effective rank). We will use:\n",
    "- Sample size: $n = 100$\n",
    "- Feature dimension: $p = 1000$ (an order of magnitude larger than $n$, providing plenty of extra degrees of freedom)\n",
    "- Covariance spectrum: For an ideal slow-decay scenario, we can take approximately constant eigenvalues. The most extreme case of “slow decay” is no decay at all – e.g. $\\lambda_i \\approx 1$ for all $i$. Here we’ll use $\\Sigma = I$ (identity covariance) as a simple case where all eigenvalues are equal (1). This means features are independent and of equal importance, and effectively the rank = p = 1000 (maximal effective rank)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also set the true signal $w^* = 0$ so that labels are pure noise. This way, any test error above the noise variance clearly indicates overfitting harm, whereas achieving test MSE $\\approx 1$ (the noise variance) would be essentially optimal (since even the best predictor, which is $0$, has to incur the noise MSE of 1). This lets us directly assess if fitting the noise has increased test error or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectation: In this benign setting, the minimum-norm interpolator should achieve near-noise-level test MSE despite zero training error. As Bartlett et al. noted, if there are many “unimportant” directions (here 1000-100 = 900 extra directions) with small variance, the fitted noise will reside in those directions and not impact prediction much​."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE = 3.35e-30\n",
      "Test MSE  = 1.085 (noise variance = 1.0)\n"
     ]
    }
   ],
   "source": [
    "# Benign scenario: p >> n and flat (slow-decay) spectrum\n",
    "n = 100\n",
    "p = 1000\n",
    "eigen_vals = np.ones(p)   # all eigenvalues = 1 (identity covariance)\n",
    "X_train, y_train, w_true, Sigma = generate_data(n, p, eigen_vals, w_star=np.zeros(p), correlated=False, random_state=1)\n",
    "w_hat = fit_min_norm(X_train, y_train)\n",
    "train_mse = compute_mse(X_train.dot(w_hat), y_train)\n",
    "# Estimate test MSE on a large test set\n",
    "n_test = 10000\n",
    "X_test = np.random.randn(n_test, p)  # since Sigma=I, we can sample standard normal for features\n",
    "y_test = X_test.dot(w_true) + np.random.randn(n_test)\n",
    "test_mse = compute_mse(X_test.dot(w_hat), y_test)\n",
    "print(f\"Train MSE = {train_mse:.2e}\")\n",
    "print(f\"Test MSE  = {test_mse:.3f} (noise variance = 1.0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results: The output shows the training MSE is essentially 0, confirming the model interpolates the training data perfectly. The test MSE should come out around ~1.1. This is very close to 1.0, the noise level. Despite fitting pure noise in the training set, the model’s predictions on new data are nearly as good as always predicting the mean (zero in this case). The small excess over 1 can be attributed to finite sample fluctuations, but it diminishes as $p$ grows even larger. We have effectively hidden the noise in the many extra dimensions of $w_{\\hat{}}$ that correspond to tiny (here zero) marginal variance in $x$, thus not hurting prediction on new samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the effect of over-parameterization more clearly, let’s vary the model dimension $p$ and see how test performance changes. We’ll keep $n=100$ fixed and use the same identity covariance (eigenvalues all 1), and examine test MSE for different ratios of $p/n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p =  100, Test MSE ≈ 27813.00\n",
      "p =  150, Test MSE ≈ 3.37\n",
      "p =  300, Test MSE ≈ 1.51\n",
      "p =  500, Test MSE ≈ 1.30\n",
      "p = 1000, Test MSE ≈ 1.11\n",
      "p = 2000, Test MSE ≈ 1.07\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "n = 100\n",
    "p_values = [100, 150, 300, 500, 1000, 2000]  # from equal to n up to 20x n\n",
    "test_mse_results = []\n",
    "for p in p_values:\n",
    "    # average test MSE over a few random trials for stability\n",
    "    trials = 3\n",
    "    mse_sum = 0.0\n",
    "    for seed in range(trials):\n",
    "        X_train, y_train, w_true, Sigma = generate_data(n, p, np.ones(p), w_star=np.zeros(p), correlated=False, random_state=seed)\n",
    "        w_hat = fit_min_norm(X_train, y_train)\n",
    "        # Compute test MSE\n",
    "        X_test = np.random.randn(10000, p)  # sampling from N(0,I) for test\n",
    "        y_test = X_test.dot(w_true) + np.random.randn(10000)\n",
    "        mse_sum += compute_mse(X_test.dot(w_hat), y_test)\n",
    "    avg_test_mse = mse_sum / trials\n",
    "    test_mse_results.append(avg_test_mse)\n",
    "    print(f\"p = {p:4d}, Test MSE ≈ {avg_test_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect a trend: when $p$ is only equal to or slightly above $n$, test MSE will be significantly higher than 1 (indicating harmful overfitting), but as $p$ grows much larger than $n$, test MSE drops toward 1. The printed results confirm this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trend confirms that significant overparameterization is crucial: as the number of features grows well beyond the sample size, the test error drops to the noise floor. This matches the theory that a large surplus of parameters (here, hundreds more dimensions than data points) is required for benign overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Role of Effective Rank: Varying the Covariance Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we investigate the effect of the covariance eigenvalue decay on benign overfitting. The theory’s characterization is in terms of the effective rank of $\\Sigma$​. Intuitively, even if $p$ is large, if the feature covariance has most of its variance concentrated in a small number of top directions (i.e. eigenvalues that decay rapidly), then there aren’t enough “small-variance” directions to hide the noise. The small eigenvalues need to decay slowly, so that there are many of them contributing a significant collective variance (making the effective rank large)​. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we will keep $n=100$ and $p=1000$ (so plenty of dimensions), but we will design different covariance spectra from slow-decaying to fast-decaying. We will use independent features for clarity (correlated=False) so that $\\Sigma = \\operatorname{diag}(\\lambda_1,\\dots,\\lambda_p)$ with the desired eigenvalues. \n",
    "\n",
    "We compare:\n",
    "- Slow Decay: Eigenvalues that decrease gradually. For example, $\\lambda_i \\propto \\frac{1}{i}$ (harmonic decay) is fairly slow. We could also consider a flat spectrum (which we did above: constant eigenvalues) as an extreme case of slow/no decay.\n",
    "- Moderate Decay: Eigenvalues might decay polynomially faster or have a mix of large and small values. For instance, $\\lambda_i \\propto \\frac{1}{i^2}$ decays faster.\n",
    "- Fast Decay: Eigenvalues that drop off very quickly, e.g. an exponential decay $\\lambda_i \\propto a^i$ for some $a<1$. This means after the first few directions, the remaining eigenvalues (and corresponding feature directions) carry negligible variance. This is a scenario with low effective rank, as most variance is in the top components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will simulate these cases and measure test MSE of the min-norm interpolator in each case. All cases use $p=1000$, $n=100$, and $w^*=0$ (pure noise labels) to isolate the effect on fitting noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected results: The test MSE will be lowest for the flattest/slowest-decaying spectrum and highest for the fastest-decaying spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrum: flat (all equal)     -> Test MSE ≈ 1.15\n",
      "Spectrum: slow (1/i)           -> Test MSE ≈ 1.36\n",
      "Spectrum: moderate (1/i^2)     -> Test MSE ≈ 2.00\n",
      "Spectrum: fast (exp decay)     -> Test MSE ≈ 4.96\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "p = 1000\n",
    "# Define different eigenvalue spectra\n",
    "eigen_spectra = {\n",
    "    \"flat (all equal)\": np.ones(p),\n",
    "    \"slow (1/i)\": 1.0 / (np.arange(p) + 1),              # harmonic decay\n",
    "    \"moderate (1/i^2)\": 1.0 / ((np.arange(p) + 1)**2),   # polynomial decay (faster)\n",
    "    \"fast (exp decay)\": 0.95**(np.arange(p))             # exponential decay with ratio 0.95\n",
    "}\n",
    "# Function to get test MSE for a given spectrum\n",
    "def get_test_mse_for_spectrum(eigs):\n",
    "    X_train, y_train, w_true, Sigma = generate_data(n, p, eigs, w_star=np.zeros(p), correlated=False)\n",
    "    w_hat = fit_min_norm(X_train, y_train)\n",
    "    # Evaluate test error on large test set\n",
    "    X_test = np.random.randn(10000, p) * np.sqrt(eigs)  # since features independent, we can multiply std dev\n",
    "    # (Alternatively, generate from Sigma via generate_data for test)\n",
    "    y_test = X_test.dot(w_true) + np.random.randn(10000)\n",
    "    test_mse = compute_mse(X_test.dot(w_hat), y_test)\n",
    "    return test_mse\n",
    "\n",
    "# Calculate test MSE for each spectrum\n",
    "for desc, eigs in eigen_spectra.items():\n",
    "    mse = get_test_mse_for_spectrum(eigs)\n",
    "    print(f\"Spectrum: {desc:20s} -> Test MSE ≈ {mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers (illustrative) show a clear pattern: when eigenvalues decay very fast (exponential case), the model that interpolates noise performs very poorly on test data (MSE several times the noise level). Conversely, with a flat or slowly decaying spectrum, the test MSE is close to 1.0 (noise level), indicating benign behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This aligns with the effective rank condition from the paper: a large effective rank (many comparably small eigenvalues) is necessary to keep the excess error from noise small​. In the exponential decay case, most of the variance is in the first few features; the model must place a lot of weight on those few directions to fit the noise (because the remaining directions have near-zero variance in $x$ and thus can’t absorb much noise without huge weights). Fitting noise in those important directions severely hurts predictions, hence the high test error. On the other hand, in the slow-decay cases, there are lots of directions with modest variance – the noise gets spread out over many directions of $w_{\\hat{}}$, each of which has little influence on the prediction, keeping test error low。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, if the covariance spectrum decays too quickly, benign overfitting fails: the interpolating solution will generalize poorly because it overfits in the important directions. Only when the spectrum has a long tail (slow decay) can the overfitting be harmless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure Cases: Violating Benign Overfitting Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Covariance Spectrum Decays Too Fast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This case is essentially the extreme of the experiment above: if the feature covariance has a rapid decay in eigenvalues, the effective rank is low and the conditions for benign overfitting are violated. We already saw an example with exponential decay. Here, let’s take an even simpler but stark example: suppose out of $p$ features, only a small handful have significant variance and the rest are negligibly small. For instance, let’s say $\\Sigma$ has 10 eigenvalues equal to 1, and the remaining $p-10$ eigenvalues equal to 0.001 (very small). This means after the first 10 principal components, the spectrum drops off sharply. We will use $n=100, p=500$ for this illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect that even though $p \\gg n$, most of those $p$ directions carry almost no variance, so effectively the model only has about 10 meaningful directions to fit the data. It will have to use those to interpolate, likely overfitting badly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE = 3.70e-29\n",
      "Test MSE  = 1.46\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "p = 500\n",
    "# Construct eigenvalues: first 10 = 1, rest = 0.001\n",
    "eigs_fast = np.array([1.0]*10 + [0.001]*(p-10))\n",
    "X_train, y_train, w_true, Sigma = generate_data(n, p, eigs_fast, w_star=np.zeros(p), correlated=False, random_state=0)\n",
    "w_hat = fit_min_norm(X_train, y_train)\n",
    "train_mse = compute_mse(X_train.dot(w_hat), y_train)\n",
    "# Test on a new dataset\n",
    "X_test, y_test, _, _ = generate_data(10000, p, eigs_fast, w_star=np.zeros(p), correlated=False, random_state=1)\n",
    "test_mse = compute_mse(X_test.dot(w_hat), y_test)\n",
    "print(f\"Train MSE = {train_mse:.2e}\")\n",
    "print(f\"Test MSE  = {test_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test MSE is larger than 1.0. This happens because effectively the model had only ~10 effective dimensions with significant variance to use for fitting – it had to contort those directions to fit random noise, which leads to large random projections on new data (high error). This confirms that a fast-decaying spectrum breaks benign overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Insufficient Overparameterization (p not ≫ n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we examine the scenario where the number of features $p$ is not significantly larger than $n$. We know from theory that we need many more parameters than data points for benign overfitting​. If $p$ is only slightly larger (or even equal or less than $n$), the model either cannot perfectly fit the data or, if it does, it won’t generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider two sub-cases:\n",
    "- Exactly $p = n$: the model has just enough parameters to potentially interpolate (if $X$ is full rank). This is the border of overparametrization.\n",
    "- Slightly above $n$: e.g. $p = 1.2n$ or $1.5n$. In such cases, there are some extra degrees of freedom but not “many more” than $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use $\\Sigma = I$ (nice flat spectrum) which on its own is a favorable scenario, and see how performance deteriorates as $p$ decreases toward $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the printout, we expect:\n",
    "- When $p < n$ (e.g. 80), the model cannot fit all training points (underparameterized). Train MSE will not be zero. The test MSE may not be as extremely large as some overfitting cases, but it will be above 1 due to the usual bias-variance tradeoff (underfitting some noise but also some signal, if any). This is not benign overfitting; it’s just the traditional case of not enough capacity.\n",
    "- When $p = n$ (100), if $X$ is full rank, the solution interpolates ($\\text{Train MSE}\\approx 0$). But as we observed, the weights can be poorly behaved. We often see very high test MSE in this scenario (the print might show a huge number or inf if numerical issues occur). This indicates a fragile solution – essentially, with no extra degrees of freedom, the interpolator might place enormous weight on directions that just barely allow fitting, leading to a disastrous test performance.\n",
    "- When $p = 120$ or $150$ (a bit over $n$), train MSE will be 0, and test MSE will be high (perhaps a few times the noise level, say 2-5). There are some extra parameters, but not enough to fully “bury” the noise harmlessly. The noise ends up spread across fewer directions, some of which inevitably have moderate variance impact on $y$, causing noticeable generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p= 80 -> Train MSE 1.94e-01, Test MSE 6.12\n",
      "p=100 -> Train MSE 1.18e-28, Test MSE 118.29\n",
      "p=120 -> Train MSE 6.36e-30, Test MSE 4.22\n",
      "p=150 -> Train MSE 4.29e-30, Test MSE 2.96\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 100\n",
    "for p in [80, 100, 120, 150]:\n",
    "    X_train, y_train, w_true, Sigma = generate_data(n, p, np.ones(p), w_star=np.zeros(p), correlated=False, random_state=42)\n",
    "    w_hat = fit_min_norm(X_train, y_train)\n",
    "    # Compute test error\n",
    "    X_test = np.random.randn(10000, p)  # since Sigma=I\n",
    "    y_test = X_test.dot(w_true) + np.random.randn(10000)\n",
    "    test_mse = compute_mse(X_test.dot(w_hat), y_test)\n",
    "    train_mse = compute_mse(X_train.dot(w_hat), y_train)\n",
    "    print(f\"p={p:3d} -> Train MSE {train_mse:.2e}, Test MSE {test_mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern is clear: if $p$ is not significantly larger than $n$, the test error is substantially worse than the noise floor (often catastrophically worse when $p$ is only equal to $n$). This reinforces that overparameterization is essential for benign overfitting​. We need a large surplus of features to achieve near-optimal generalization when interpolating noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Correlated Features (Violating Independence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theory and our previous simulations assumed features are roughly independent (or at least that the covariance eigenstructure is not pathological). What if features are highly correlated with each other? Correlation among features effectively reduces the dimensionality of the data — even if $p$ is large, if many features are redundant or linearly dependent, the true number of independent directions is smaller. This can undermine the overparameterization and effective rank conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test a scenario with **strong feature correlation**. One way to induce this is to use a covariance matrix with high off-diagonal values. For example, consider an AR(1) covariance where features have correlation $\\rho^{|i-j|}$ between feature $i$ and $j$. If $\\rho$ is close to 1, adjacent features are highly collinear. We use an AR(1) model with $\\rho = 0.9$ for a pronounced correlation. We’ll compare it to an independent feature case (identity covariance) with the same $n$ and $p$ to isolate the effect of correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take $n=100, p=300$ and compare:\n",
    "- Independent features: $\\Sigma = I_{300}$.\n",
    "- Correlated features: $\\Sigma_{ij} = 0.9^{|i-j|}$ (AR(1) covariance with strong correlation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We keep $w^*=0$ and measure the test MSE for both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected outcome: The test MSE with correlated features will be worse than with independent features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE (independent features)  = 1.45\n",
      "Test MSE (correlated features)   = 3.35\n"
     ]
    }
   ],
   "source": [
    "import numpy.linalg as LA\n",
    "\n",
    "def ar1_cov(p, rho):\n",
    "    # Construct an AR(1) covariance matrix of size p with correlation rho\n",
    "    return np.array([[rho**abs(i-j) for j in range(p)] for i in range(p)])\n",
    "\n",
    "n = 100\n",
    "p = 300\n",
    "# Independent case:\n",
    "Sigma_indep = np.eye(p)\n",
    "# Correlated case (AR(1) with rho=0.9):\n",
    "Sigma_corr = ar1_cov(p, rho=0.9)\n",
    "# Generate data for each case\n",
    "X_i, y_i, w_true, _ = generate_data(n, p, np.ones(p), w_star=np.zeros(p), correlated=False, random_state=0)\n",
    "X_c, y_c, w_true, _ = generate_data(n, p, LA.eigvalsh(Sigma_corr)[::-1], w_star=np.zeros(p), correlated=True, random_state=0)\n",
    "# Note: We used LA.eigvalsh to get eigenvalues of Sigma_corr, then generate_data with correlated=True to actually produce data with that covariance\n",
    "# Fit models\n",
    "w_hat_i = fit_min_norm(X_i, y_i)\n",
    "w_hat_c = fit_min_norm(X_c, y_c)\n",
    "# Compute test errors\n",
    "X_test_i, y_test_i, _, _ = generate_data(10000, p, np.ones(p), w_star=np.zeros(p), correlated=False, random_state=1)\n",
    "X_test_c, y_test_c, _, _ = generate_data(10000, p, LA.eigvalsh(Sigma_corr)[::-1], w_star=np.zeros(p), correlated=True, random_state=1)\n",
    "test_mse_i = compute_mse(X_test_i.dot(w_hat_i), y_test_i)\n",
    "test_mse_c = compute_mse(X_test_c.dot(w_hat_c), y_test_c)\n",
    "print(f\"Test MSE (independent features)  = {test_mse_i:.2f}\")\n",
    "print(f\"Test MSE (correlated features)   = {test_mse_c:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlated case (AR(0.9)) had test MSE 3.35, notably higher than the MSE of the independent case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason: with $\\rho=0.9$, the features are highly collinear, effectively reducing the effective dimensionality of the data. Indeed, the AR(1) covariance has a decaying spectrum – one large eigenvalue and a bunch of smaller ones (we could examine $\\operatorname{eigvalsh}(Sigma_corr)$ to see the distribution). The effective rank is lower than 300 in this case, violating the ideal conditions. As a result, the model cannot hide the noise as effectively and ends up with higher prediction error. In other words, correlation among features can negate the benefits of high nominal dimension by collapsing variance into fewer directions, making overfitting harmful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiments show that a highly over-parameterized linear model with a slowly decaying covariance spectrum can perfectly interpolate training noise while keeping test error near the irreducible noise level—illustrating benign overfitting. However, when the effective rank is reduced (via rapid eigenvalue decay, strong feature correlations, or insufficient extra dimensions), the minimal-norm solution becomes unstable and test error increases sharply. These results underscore the crucial balance between model capacity and data covariance in achieving benign overfitting, offering insight into why highly over-parameterized models (like deep networks) can generalize well despite fitting noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
